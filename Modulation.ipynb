{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from konlpy.tag import Okt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "#LSTM Training, Test\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#불용어 사전 제작\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preprocessing\n",
    "def preprocessing(data):\n",
    "# 데이터 중복 값 확인 및 제거\n",
    "# document: 약 4,000개 중복(총150,000개 - 146,182개), \n",
    "# label 2값 확인(0, 1만 가지기 때문에)\n",
    "    \n",
    "    # 중복 여부 검사, nunique()는 중복 제외하고 표시. list와 유사\n",
    "    data['document'].nunique(), data['label'].nunique()\n",
    "    \n",
    "    # document 컬럼에서 중복인 내용이 있으면 중복 제거\n",
    "    data.drop_duplicates(subset=['document'], inplace=True)\n",
    "    data = data.dropna(how='any') # Null값이 존재하는 행 제거\n",
    "    \n",
    "    # 한글과 공백을 제외하고 모두 제거하는 정규 표현식 사용\n",
    "    data['document'] = data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\")\n",
    "    \n",
    "    # 다시한번 Null값이 존재하는지 확인\n",
    "    #공백만 있거나 빈 값을 가진 행이 있으면 Null로 변경\n",
    "    data['document'] = data['document'].str.replace('^ +', \"\") \n",
    "    data['document'].replace('', np.nan, inplace=True) #Null값으로 변환\n",
    "    \n",
    "    # 데이터의 null 행 제거\n",
    "    data = data.dropna(how='any')\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data tokenization\n",
    "def train_tokenizer(data):\n",
    "# 입력값: Null값이 제거된 데이터셋 / 출력값: 불용어를 제거한 후 Null값도 제거한 데이터셋\n",
    "# 기능: 불용어 제거, 단어 빈도수가 2회 이하인 단어 수를 찾아내고 공백 제거(공백 제거 목적)\n",
    "    \n",
    "    X_data=[]\n",
    "    for sentence in data['document']:\n",
    "        # 형태소 분석기(Okt())에서 토큰화(한글은 띄어쓰기) 실행. \n",
    "        # stem=True로 일정 수준 정규화(동사,명사화)\n",
    "        X_tmp = okt.morphs(sentence, stem=True)\n",
    "        X_tmp = [word for word in X_tmp if not word in stopwords] # 불용어 사전에 없으면 리스트에 추가\n",
    "        X_data.append(X_tmp)\n",
    "\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(X_data)\n",
    "    \n",
    "    # 등장 빈도수가 3회 미만인 단어들이 이 데이터에서 얼만큼 비중을 차지하는지 확인\n",
    "    threshold = 3 # 단어의 등장 빈도수 기준\n",
    "    rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "    total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "    rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 빈도수 총 합\n",
    "    total_cnt = len(tokenizer.word_index) #단어의 수\n",
    "    \n",
    "    # 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "    for key, value in tokenizer.word_counts.items():\n",
    "        total_freq = total_freq + value\n",
    "\n",
    "        # 단어 빈도수가 threshold보다 작을 경우\n",
    "        if(value < threshold):\n",
    "            rare_cnt = rare_cnt + 1\n",
    "            rare_freq = rare_freq + value\n",
    "\n",
    "    # 빈도수가 2회 이하인 단어들은 제외\n",
    "    # 0번일 경우를 고려하여 크기는 +1을 해준다.\n",
    "    voca_size = total_cnt - rare_cnt + 1\n",
    "    \n",
    "    # 토큰화(단어단위로 쪼갬)\n",
    "    tokenizer = Tokenizer(voca_size)\n",
    "    # 단어에 숫자(인덱스)를 부여 \n",
    "    tokenizer.fit_on_texts(X_data)\n",
    "    \n",
    "    y_data = np.array(data['label'])\n",
    "    global tk\n",
    "    tk = tokenizer\n",
    "    X_data, y_data = train_padding(X_data, y_data)\n",
    "    \n",
    "    return X_data, y_data, voca_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data padding\n",
    "def train_padding(X_data, y_data):\n",
    "    \n",
    "    # texts_to_sequences, 단어에 순번을 지정, 0 ~ 19,415번 단어가 있음\n",
    "    X_data = tk.texts_to_sequences(X_data) \n",
    "\n",
    "    #empty samples 제거\n",
    "    # 단어가 1개 미만의 값이 없는 데이터 제거\n",
    "    drop_data = [index for index, sentence in enumerate(X_data) if len(sentence) < 1] \n",
    "\n",
    "    # 빈 샘플 제거\n",
    "    X_data = np.delete(X_data, drop_data, axis=0) # X_data에서 drop_data을 사용해서 제거\n",
    "    y_data = np.delete(y_data, drop_data, axis=0)\n",
    "    \n",
    "    # 전체 훈련 데이터중 94%가 길이가 30 이하이므로 모든 샘플의 길이를 30으로 조정\n",
    "    X_data = pad_sequences(X_data, maxlen = 30)\n",
    "    \n",
    "    return X_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data tokenization\n",
    "def test_tokenizer(data): \n",
    "# 입력값: Null값이 제거된 데이터셋 / 출력값: 불용어를 제거한 후 Null값도 제거한 데이터셋\n",
    "# 기능: 불용어 제거, 단어 빈도수가 2회 이하인 단어 수를 찾아내고 공백 제거(공백 제거 목적)\n",
    "\n",
    "    X_data=[]\n",
    "    for sentence in data['document']:\n",
    "        # 형태소 분석기(Okt())에서 토큰화(한글은 띄어쓰기) 실행. stem=True로 일정 수준 정규화(동사,명사화)\n",
    "        X_tmp = okt.morphs(sentence, stem=True)\n",
    "        X_tmp = [word for word in X_tmp if not word in stopwords] # 불용어 사전에 없으면 리스트에 추가\n",
    "        X_data.append(X_tmp)\n",
    "\n",
    "    y_data = np.array(data['label'])\n",
    "\n",
    "    X_data = test_padding(X_data)\n",
    "    \n",
    "    return X_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data padding\n",
    "def test_padding(X_data):\n",
    "\n",
    "    # texts_to_sequences, 단어에 순번을 지정, 0 ~ 19,415번 단어가 있음\n",
    "    X_data = tk.texts_to_sequences(X_data) \n",
    "\n",
    "    #전체 훈련 데이터중 94%가 길이가 30 이하이므로 모든 샘플의 길이를 30으로 조정\n",
    "    X_data = pad_sequences(X_data, maxlen = 30)\n",
    "\n",
    "    return X_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data modeling\n",
    "def lstm_modeling(X_data, y_data, X_data2, y_data2, voca_size): #입력값: / 출력값: /함수의 기능:\n",
    "\n",
    "    model = Sequential()\n",
    "    # Embedding(number of samples, size of vector, input length) \n",
    "    model.add(Embedding(voca_size, 100)) # vocab_size 개수만큼, 벡터 사이즈 100차원\n",
    "    model.add(LSTM(128)) # LSTM cell 개수\n",
    "    \n",
    "    # 출력층 뉴런 1개, 활성화 함수 sigmoid(binary classification이기 때문)\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # 검증 데이터 손실이 증가하면, 과적합 위험. 검증 데이터 손실이 4회 증가하면 학습을 조기 종료\n",
    "    # ModelCheckpoint를 사용하여 검증 데이터의 정확도(val_acc)가 이전보다 좋아질 경우에만 모델 저장\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "    mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "    # RMSProp, 세밀하게 학습하지만 상황에 따라 정도를 정하기 위함\n",
    "    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "    history = model.fit(X_data, y_data, epochs=15, callbacks=[es, mc], batch_size=60, validation_split=0.2)\n",
    "\n",
    "    loaded_model = load_model('best_model.h5')\n",
    "    print(\"\\n 테스트 정확도 : %0.4f\" % (loaded_model.evaluate(X_data2, y_data2)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data prediction\n",
    "def predict(new_sentence):\n",
    "    loaded_model = load_model('best_model.h5')\n",
    "    new_sentence = okt.morphs(new_sentence, stem=True) # 토큰화\n",
    "    new_sentence = [word for word in new_sentence if not word in stopwords] # 불용어 \n",
    "    pad_new = test_padding([new_sentence])\n",
    "    score = float(loaded_model.predict(pad_new)) # 예측\n",
    "    if(score > 0.5):\n",
    "        print(\"{:.2f}% 확률로 긍정 리뷰입니다.\\n\".format(score * 100))\n",
    "    else:\n",
    "        print(\"{:.2f}% 확률로 부정 리뷰입니다.\\n\".format((1 - score) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_table('C:/Users/CJ/Documents/project2/ratings_train.txt')\n",
    "test_data = pd.read_table('C:/Users/CJ/Documents/project2/ratings_test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preprocessing\n",
    "train_data = preprocessing(train_data)\n",
    "test_data = preprocessing(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data tokenization\n",
    "X_train, y_train, voca_size = train_tokenizer(train_data)\n",
    "X_test, y_test = test_tokenizer(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1934/1936 [============================>.] - ETA: 0s - loss: 0.3892 - acc: 0.8238\n",
      "Epoch 00001: val_acc improved from -inf to 0.84394, saving model to best_model.h5\n",
      "1936/1936 [==============================] - 67s 35ms/step - loss: 0.3892 - acc: 0.8238 - val_loss: 0.3535 - val_acc: 0.8439\n",
      "Epoch 2/15\n",
      "1935/1936 [============================>.] - ETA: 0s - loss: 0.3272 - acc: 0.8579\n",
      "Epoch 00002: val_acc improved from 0.84394 to 0.85585, saving model to best_model.h5\n",
      "1936/1936 [==============================] - 67s 35ms/step - loss: 0.3272 - acc: 0.8579 - val_loss: 0.3367 - val_acc: 0.8559\n",
      "Epoch 3/15\n",
      "1935/1936 [============================>.] - ETA: 0s - loss: 0.3026 - acc: 0.8717\n",
      "Epoch 00003: val_acc improved from 0.85585 to 0.86002, saving model to best_model.h5\n",
      "1936/1936 [==============================] - 60s 31ms/step - loss: 0.3025 - acc: 0.8717 - val_loss: 0.3296 - val_acc: 0.8600\n",
      "Epoch 4/15\n",
      "1936/1936 [==============================] - ETA: 0s - loss: 0.2834 - acc: 0.8823\n",
      "Epoch 00004: val_acc improved from 0.86002 to 0.86009, saving model to best_model.h5\n",
      "1936/1936 [==============================] - 60s 31ms/step - loss: 0.2834 - acc: 0.8823 - val_loss: 0.3315 - val_acc: 0.8601\n",
      "Epoch 5/15\n",
      "1935/1936 [============================>.] - ETA: 0s - loss: 0.2675 - acc: 0.8903\n",
      "Epoch 00005: val_acc did not improve from 0.86009\n",
      "1936/1936 [==============================] - 59s 30ms/step - loss: 0.2675 - acc: 0.8903 - val_loss: 0.3319 - val_acc: 0.8587\n",
      "Epoch 6/15\n",
      "1935/1936 [============================>.] - ETA: 0s - loss: 0.2525 - acc: 0.8981\n",
      "Epoch 00006: val_acc did not improve from 0.86009\n",
      "1936/1936 [==============================] - 65s 33ms/step - loss: 0.2525 - acc: 0.8981 - val_loss: 0.3330 - val_acc: 0.8583\n",
      "Epoch 7/15\n",
      "1935/1936 [============================>.] - ETA: 0s - loss: 0.2373 - acc: 0.9045\n",
      "Epoch 00007: val_acc did not improve from 0.86009\n",
      "1936/1936 [==============================] - 64s 33ms/step - loss: 0.2373 - acc: 0.9045 - val_loss: 0.3379 - val_acc: 0.8554\n",
      "Epoch 00007: early stopping\n",
      "1527/1527 [==============================] - 8s 5ms/step - loss: 0.3381 - acc: 0.8557\n",
      "\n",
      " 테스트 정확도 : 0.8557\n"
     ]
    }
   ],
   "source": [
    "# lstm_modeling(X_train, y_train, voca_size)\n",
    "lstm_modeling(X_train, y_train, X_test, y_test, voca_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001EB5B826E50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "94.95% 확률로 긍정 리뷰입니다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict('재밌어요')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001EB5B8D3310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "66.02% 확률로 긍정 리뷰입니다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict('한번쯤은 볼만한 영화')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001EB5AA49160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "98.40% 확률로 부정 리뷰입니다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict('쓰레기')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001EB5A9A33A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "55.14% 확률로 부정 리뷰입니다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict('보라고 하고싶진 않지만 한번은 보세요')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001EB5B86C940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "84.58% 확률로 긍정 리뷰입니다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict('꼭 보고싶다면 말리진 않을게요')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001EB5A98AD30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "96.33% 확률로 부정 리뷰입니다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict('이걸 돈주고 본다고?')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
